{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "C:\\Users\\nikhil\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py:936: DeprecationWarning: builtin type EagerTensor has no __module__ attribute\n",
      "  EagerTensor = c_api.TFE_Py_InitEagerTensor(_EagerTensorBase)\n",
      "C:\\Users\\nikhil\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\util\\tf_inspect.py:75: DeprecationWarning: inspect.getargspec() is deprecated, use inspect.signature() or inspect.getfullargspec()\n",
      "  return _inspect.getargspec(target)\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append('../')\n",
    "import pandas as pd\n",
    "import warnings\n",
    "from copy import deepcopy\n",
    "from time import sleep\n",
    "from PIL import Image, ImageTk\n",
    "import matplotlib\n",
    "import datetime\n",
    "matplotlib.use('Agg')\n",
    "from matplotlib import pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set(style=\"darkgrid\")\n",
    "import tkinter as tk\n",
    "from tkinter import font  as tkfont \n",
    "from tkinter.ttk import Combobox\n",
    "from time import sleep\n",
    "from modules.basic import *\n",
    "from modules.forecast import *\n",
    "import os\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "stocks_data = pd.read_csv(\"../stocks.csv\")\n",
    "stocks_data['Date'] = stocks_data['Date'].astype('datetime64[ns]')\n",
    "stocks_data = stocks_data.set_index('Date')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "stocks_data_2016 = stocks_data['2016']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "companies = deepcopy(stocks_data_2016.Company)\n",
    "companies.drop_duplicates(inplace=True)\n",
    "companies.reset_index(drop=True,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "comp_list = list(companies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "comp_stocks=pd.read_csv(\"../datasets/filtered_companies.csv\")\n",
    "comp_stocks['Symbol']=comp_stocks.Symbol.str.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iphone\n"
     ]
    }
   ],
   "source": [
    "data_words = pd.read_csv(\"../datasets/words_dates_list_cw.csv\")\n",
    "data_words.drop_duplicates(inplace=True)\n",
    "data_words.reset_index(drop=True,inplace=True)\n",
    "key_count = data_words.groupby([\"keyword\"], sort=False).count().reset_index()\n",
    "key_count = key_count.loc[key_count['freq'] >= 5]\n",
    "word_data = deepcopy(key_count.keyword)\n",
    "#word_list = sorted(map(str, list(word_data)))\n",
    "word_list = list(map(str, list(word_data)))\n",
    "for i in word_list:\n",
    "    if 'phone' in i:\n",
    "        print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "scaler = MinMaxScaler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Signal extention\n",
    "def extend_signal(data):\n",
    "    i = 0\n",
    "    while(i < len(data) - 1 and pd.isna(data[i])):\n",
    "        i += 1\n",
    "    j = 0\n",
    "    while(i > 0):\n",
    "        data[i-1] = data[i+j]\n",
    "        if i+j+2 <= len(data):\n",
    "            j += 2\n",
    "        i -= 1\n",
    "    return data.fillna(0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "            freq\n",
      "Date            \n",
      "2016-01-13    10\n",
      "2016-01-20     8\n",
      "2016-01-27     7\n",
      "2016-02-03     8\n",
      "2016-02-10     9\n",
      "2016-02-17    11\n",
      "2016-02-24     6\n",
      "2016-03-02    12\n",
      "2016-03-09     6\n",
      "2016-03-16    11\n",
      "2016-03-23    11\n",
      "2016-03-30     9\n",
      "2016-04-06    10\n",
      "2016-04-13     9\n",
      "2016-04-20    10\n",
      "2016-04-27    10\n",
      "2016-05-04     7\n",
      "2016-05-11     7\n",
      "2016-05-18     9\n",
      "2016-05-25     7\n",
      "2016-06-01     7\n",
      "2016-06-08    13\n",
      "2016-06-15     8\n",
      "2016-06-22     5\n",
      "2016-06-29     9\n",
      "2016-07-06     8\n",
      "2016-07-13     6\n",
      "2016-07-20     6\n",
      "2016-07-27    12\n",
      "2016-08-03    11\n",
      "2016-08-10    10\n",
      "2016-08-17     7\n",
      "2016-08-24    10\n",
      "2016-08-31     7\n",
      "2016-09-07     3\n",
      "2016-09-14    11\n",
      "2016-09-21     6\n",
      "2016-09-28    13\n",
      "2016-10-05    11\n",
      "2016-10-12    11\n",
      "2016-10-19     7\n",
      "2016-10-26    11\n",
      "2016-11-02     7\n",
      "2016-11-09     9\n",
      "2016-11-16    11\n",
      "2016-11-23     5\n",
      "2016-11-30     8\n",
      "2016-12-07    10\n",
      "2016-12-14     9\n",
      "2016-12-21     5\n",
      "2016-12-28     2\n",
      "\n",
      "Granger Causality\n",
      "number of lags (no zero) 1\n",
      "ssr based F test:         F=8.8814  , p=0.0032  , df_denom=208, df_num=1\n",
      "ssr based chi2 test:   chi2=9.0095  , p=0.0027  , df=1\n",
      "likelihood ratio test: chi2=8.8225  , p=0.0030  , df=1\n",
      "parameter F test:         F=8.8814  , p=0.0032  , df_denom=208, df_num=1\n",
      "\n",
      "Granger Causality\n",
      "number of lags (no zero) 2\n",
      "ssr based F test:         F=3.4164  , p=0.0347  , df_denom=205, df_num=2\n",
      "ssr based chi2 test:   chi2=6.9994  , p=0.0302  , df=2\n",
      "likelihood ratio test: chi2=6.8853  , p=0.0320  , df=2\n",
      "parameter F test:         F=3.4164  , p=0.0347  , df_denom=205, df_num=2\n",
      "\n",
      "Granger Causality\n",
      "number of lags (no zero) 3\n",
      "ssr based F test:         F=4.2908  , p=0.0058  , df_denom=202, df_num=3\n",
      "ssr based chi2 test:   chi2=13.3183 , p=0.0040  , df=3\n",
      "likelihood ratio test: chi2=12.9112 , p=0.0048  , df=3\n",
      "parameter F test:         F=4.2908  , p=0.0058  , df_denom=202, df_num=3\n",
      "\n",
      "Granger Causality\n",
      "number of lags (no zero) 4\n",
      "ssr based F test:         F=0.9977  , p=0.4099  , df_denom=199, df_num=4\n",
      "ssr based chi2 test:   chi2=4.1714  , p=0.3833  , df=4\n",
      "likelihood ratio test: chi2=4.1301  , p=0.3887  , df=4\n",
      "parameter F test:         F=0.9977  , p=0.4099  , df_denom=199, df_num=4\n",
      "\n",
      "Granger Causality\n",
      "number of lags (no zero) 5\n",
      "ssr based F test:         F=0.5657  , p=0.7262  , df_denom=196, df_num=5\n",
      "ssr based chi2 test:   chi2=2.9874  , p=0.7019  , df=5\n",
      "likelihood ratio test: chi2=2.9660  , p=0.7052  , df=5\n",
      "parameter F test:         F=0.5657  , p=0.7262  , df_denom=196, df_num=5\n",
      "\n",
      "Granger Causality\n",
      "number of lags (no zero) 6\n",
      "ssr based F test:         F=0.5388  , p=0.7783  , df_denom=193, df_num=6\n",
      "ssr based chi2 test:   chi2=3.4504  , p=0.7506  , df=6\n",
      "likelihood ratio test: chi2=3.4218  , p=0.7543  , df=6\n",
      "parameter F test:         F=0.5388  , p=0.7783  , df_denom=193, df_num=6\n",
      "\n",
      "Granger Causality\n",
      "number of lags (no zero) 7\n",
      "ssr based F test:         F=1.4238  , p=0.1978  , df_denom=190, df_num=7\n",
      "ssr based chi2 test:   chi2=10.7532 , p=0.1498  , df=7\n",
      "likelihood ratio test: chi2=10.4807 , p=0.1629  , df=7\n",
      "parameter F test:         F=1.4238  , p=0.1978  , df_denom=190, df_num=7\n",
      "\n",
      "Granger Causality\n",
      "number of lags (no zero) 8\n",
      "ssr based F test:         F=1.0815  , p=0.3779  , df_denom=187, df_num=8\n",
      "ssr based chi2 test:   chi2=9.4382  , p=0.3067  , df=8\n",
      "likelihood ratio test: chi2=9.2263  , p=0.3236  , df=8\n",
      "parameter F test:         F=1.0815  , p=0.3779  , df_denom=187, df_num=8\n",
      "\n",
      "Granger Causality\n",
      "number of lags (no zero) 9\n",
      "ssr based F test:         F=0.9399  , p=0.4918  , df_denom=184, df_num=9\n",
      "ssr based chi2 test:   chi2=9.3326  , p=0.4072  , df=9\n",
      "likelihood ratio test: chi2=9.1244  , p=0.4259  , df=9\n",
      "parameter F test:         F=0.9399  , p=0.4918  , df_denom=184, df_num=9\n",
      "\n",
      "Granger Causality\n",
      "number of lags (no zero) 10\n",
      "ssr based F test:         F=1.0380  , p=0.4134  , df_denom=181, df_num=10\n",
      "ssr based chi2 test:   chi2=11.5845 , p=0.3138  , df=10\n",
      "likelihood ratio test: chi2=11.2645 , p=0.3373  , df=10\n",
      "parameter F test:         F=1.0380  , p=0.4134  , df_denom=181, df_num=10\n",
      "\n",
      "Granger Causality\n",
      "number of lags (no zero) 11\n",
      "ssr based F test:         F=0.9046  , p=0.5372  , df_denom=178, df_num=11\n",
      "ssr based chi2 test:   chi2=11.2362 , p=0.4237  , df=11\n",
      "likelihood ratio test: chi2=10.9333 , p=0.4489  , df=11\n",
      "parameter F test:         F=0.9046  , p=0.5372  , df_denom=178, df_num=11\n",
      "\n",
      "Granger Causality\n",
      "number of lags (no zero) 12\n",
      "ssr based F test:         F=0.9705  , p=0.4789  , df_denom=175, df_num=12\n",
      "ssr based chi2 test:   chi2=13.3096 , p=0.3469  , df=12\n",
      "likelihood ratio test: chi2=12.8855 , p=0.3774  , df=12\n",
      "parameter F test:         F=0.9705  , p=0.4789  , df_denom=175, df_num=12\n",
      "\n",
      "Granger Causality\n",
      "number of lags (no zero) 13\n",
      "ssr based F test:         F=0.8569  , p=0.5996  , df_denom=172, df_num=13\n",
      "ssr based chi2 test:   chi2=12.8879 , p=0.4565  , df=13\n",
      "likelihood ratio test: chi2=12.4878 , p=0.4881  , df=13\n",
      "parameter F test:         F=0.8569  , p=0.5996  , df_denom=172, df_num=13\n",
      "\n",
      "Granger Causality\n",
      "number of lags (no zero) 14\n",
      "ssr based F test:         F=0.7208  , p=0.7515  , df_denom=169, df_num=14\n",
      "ssr based chi2 test:   chi2=11.8236 , p=0.6205  , df=14\n",
      "likelihood ratio test: chi2=11.4841 , p=0.6477  , df=14\n",
      "parameter F test:         F=0.7208  , p=0.7515  , df_denom=169, df_num=14\n",
      "\n",
      "Granger Causality\n",
      "number of lags (no zero) 15\n",
      "ssr based F test:         F=0.8111  , p=0.6642  , df_denom=166, df_num=15\n",
      "ssr based chi2 test:   chi2=14.4387 , p=0.4926  , df=15\n",
      "likelihood ratio test: chi2=13.9341 , p=0.5305  , df=15\n",
      "parameter F test:         F=0.8111  , p=0.6642  , df_denom=166, df_num=15\n",
      "\n",
      "Granger Causality\n",
      "number of lags (no zero) 16\n",
      "ssr based F test:         F=0.7317  , p=0.7587  , df_denom=163, df_num=16\n",
      "ssr based chi2 test:   chi2=14.0767 , p=0.5930  , df=16\n",
      "likelihood ratio test: chi2=13.5942 , p=0.6289  , df=16\n",
      "parameter F test:         F=0.7317  , p=0.7587  , df_denom=163, df_num=16\n",
      "\n",
      "Granger Causality\n",
      "number of lags (no zero) 17\n",
      "ssr based F test:         F=0.7945  , p=0.6977  , df_denom=160, df_num=17\n",
      "ssr based chi2 test:   chi2=16.4603 , p=0.4915  , df=17\n",
      "likelihood ratio test: chi2=15.8024 , p=0.5379  , df=17\n",
      "parameter F test:         F=0.7945  , p=0.6977  , df_denom=160, df_num=17\n",
      "\n",
      "Granger Causality\n",
      "number of lags (no zero) 18\n",
      "ssr based F test:         F=0.7171  , p=0.7900  , df_denom=157, df_num=18\n",
      "ssr based chi2 test:   chi2=15.9505 , p=0.5960  , df=18\n",
      "likelihood ratio test: chi2=15.3286 , p=0.6393  , df=18\n",
      "parameter F test:         F=0.7171  , p=0.7900  , df_denom=157, df_num=18\n",
      "\n",
      "Granger Causality\n",
      "number of lags (no zero) 19\n",
      "ssr based F test:         F=0.7492  , p=0.7628  , df_denom=154, df_num=19\n",
      "ssr based chi2 test:   chi2=17.8393 , p=0.5332  , df=19\n",
      "likelihood ratio test: chi2=17.0624 , p=0.5856  , df=19\n",
      "parameter F test:         F=0.7492  , p=0.7628  , df_denom=154, df_num=19\n",
      "\n",
      "Granger Causality\n",
      "number of lags (no zero) 20\n",
      "ssr based F test:         F=0.7779  , p=0.7367  , df_denom=151, df_num=20\n",
      "ssr based chi2 test:   chi2=19.7828 , p=0.4716  , df=20\n",
      "likelihood ratio test: chi2=18.8287 , p=0.5330  , df=20\n",
      "parameter F test:         F=0.7779  , p=0.7367  , df_denom=151, df_num=20\n",
      "\n",
      "Granger Causality\n",
      "number of lags (no zero) 21\n",
      "ssr based F test:         F=0.7794  , p=0.7410  , df_denom=148, df_num=21\n",
      "ssr based chi2 test:   chi2=21.1220 , p=0.4515  , df=21\n",
      "likelihood ratio test: chi2=20.0336 , p=0.5191  , df=21\n",
      "parameter F test:         F=0.7794  , p=0.7410  , df_denom=148, df_num=21\n",
      "\n",
      "Granger Causality\n",
      "number of lags (no zero) 22\n",
      "ssr based F test:         F=0.5827  , p=0.9299  , df_denom=145, df_num=22\n",
      "ssr based chi2 test:   chi2=16.7969 , p=0.7745  , df=22\n",
      "likelihood ratio test: chi2=16.0955 , p=0.8111  , df=22\n",
      "parameter F test:         F=0.5827  , p=0.9299  , df_denom=145, df_num=22\n",
      "\n",
      "Granger Causality\n",
      "number of lags (no zero) 23\n",
      "ssr based F test:         F=0.5978  , p=0.9248  , df_denom=142, df_num=23\n",
      "ssr based chi2 test:   chi2=18.2991 , p=0.7411  , df=23\n",
      "likelihood ratio test: chi2=17.4666 , p=0.7857  , df=23\n",
      "parameter F test:         F=0.5978  , p=0.9248  , df_denom=142, df_num=23\n",
      "\n",
      "Granger Causality\n",
      "number of lags (no zero) 24\n",
      "ssr based F test:         F=0.5903  , p=0.9336  , df_denom=139, df_num=24\n",
      "ssr based chi2 test:   chi2=19.1624 , p=0.7433  , df=24\n",
      "likelihood ratio test: chi2=18.2475 , p=0.7908  , df=24\n",
      "parameter F test:         F=0.5903  , p=0.9336  , df_denom=139, df_num=24\n",
      "\n",
      "Granger Causality\n",
      "number of lags (no zero) 25\n",
      "ssr based F test:         F=0.5946  , p=0.9348  , df_denom=136, df_num=25\n",
      "ssr based chi2 test:   chi2=20.4395 , p=0.7234  , df=25\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "likelihood ratio test: chi2=19.3977 , p=0.7777  , df=25\n",
      "parameter F test:         F=0.5946  , p=0.9348  , df_denom=136, df_num=25\n",
      "\n",
      "Granger Causality\n",
      "number of lags (no zero) 26\n",
      "ssr based F test:         F=0.5318  , p=0.9686  , df_denom=133, df_num=26\n",
      "ssr based chi2 test:   chi2=19.3383 , p=0.8218  , df=26\n",
      "likelihood ratio test: chi2=18.3977 , p=0.8608  , df=26\n",
      "parameter F test:         F=0.5318  , p=0.9686  , df_denom=133, df_num=26\n",
      "\n",
      "Granger Causality\n",
      "number of lags (no zero) 27\n",
      "ssr based F test:         F=0.6624  , p=0.8933  , df_denom=130, df_num=27\n",
      "ssr based chi2 test:   chi2=25.4515 , p=0.5492  , df=27\n",
      "likelihood ratio test: chi2=23.8464 , p=0.6388  , df=27\n",
      "parameter F test:         F=0.6624  , p=0.8933  , df_denom=130, df_num=27\n",
      "\n",
      "Granger Causality\n",
      "number of lags (no zero) 28\n",
      "ssr based F test:         F=0.6791  , p=0.8827  , df_denom=127, df_num=28\n",
      "ssr based chi2 test:   chi2=27.5494 , p=0.4885  , df=28\n",
      "likelihood ratio test: chi2=25.6722 , p=0.5911  , df=28\n",
      "parameter F test:         F=0.6791  , p=0.8827  , df_denom=127, df_num=28\n",
      "\n",
      "Granger Causality\n",
      "number of lags (no zero) 29\n",
      "ssr based F test:         F=0.6464  , p=0.9134  , df_denom=124, df_num=29\n",
      "ssr based chi2 test:   chi2=27.6638 , p=0.5359  , df=29\n",
      "likelihood ratio test: chi2=25.7623 , p=0.6382  , df=29\n",
      "parameter F test:         F=0.6464  , p=0.9134  , df_denom=124, df_num=29\n",
      "\n",
      "Granger Causality\n",
      "number of lags (no zero) 30\n",
      "ssr based F test:         F=0.9624  , p=0.5294  , df_denom=121, df_num=30\n",
      "ssr based chi2 test:   chi2=43.4260 , p=0.0537  , df=30\n",
      "likelihood ratio test: chi2=38.9453 , p=0.1270  , df=30\n",
      "parameter F test:         F=0.9624  , p=0.5294  , df_denom=121, df_num=30\n",
      "\n",
      "Granger Causality\n",
      "number of lags (no zero) 31\n",
      "ssr based F test:         F=0.9541  , p=0.5428  , df_denom=118, df_num=31\n",
      "ssr based chi2 test:   chi2=45.3680 , p=0.0462  , df=31\n",
      "likelihood ratio test: chi2=40.4834 , p=0.1185  , df=31\n",
      "parameter F test:         F=0.9541  , p=0.5428  , df_denom=118, df_num=31\n",
      "\n",
      "Granger Causality\n",
      "number of lags (no zero) 32\n",
      "ssr based F test:         F=0.9770  , p=0.5114  , df_denom=115, df_num=32\n",
      "ssr based chi2 test:   chi2=48.9340 , p=0.0282  , df=32\n",
      "likelihood ratio test: chi2=43.2859 , p=0.0879  , df=32\n",
      "parameter F test:         F=0.9770  , p=0.5114  , df_denom=115, df_num=32\n",
      "\n",
      "Granger Causality\n",
      "number of lags (no zero) 33\n",
      "ssr based F test:         F=0.9846  , p=0.5017  , df_denom=112, df_num=33\n",
      "ssr based chi2 test:   chi2=51.9275 , p=0.0192  , df=33\n",
      "likelihood ratio test: chi2=45.5945 , p=0.0711  , df=33\n",
      "parameter F test:         F=0.9846  , p=0.5017  , df_denom=112, df_num=33\n",
      "\n",
      "Granger Causality\n",
      "number of lags (no zero) 34\n",
      "ssr based F test:         F=1.0516  , p=0.4091  , df_denom=109, df_num=34\n",
      "ssr based chi2 test:   chi2=58.3864 , p=0.0058  , df=34\n",
      "likelihood ratio test: chi2=50.4958 , p=0.0341  , df=34\n",
      "parameter F test:         F=1.0516  , p=0.4091  , df_denom=109, df_num=34\n",
      "\n",
      "Granger Causality\n",
      "number of lags (no zero) 35\n",
      "ssr based F test:         F=0.9846  , p=0.5038  , df_denom=106, df_num=35\n",
      "ssr based chi2 test:   chi2=57.5432 , p=0.0095  , df=35\n",
      "likelihood ratio test: chi2=49.8238 , p=0.0498  , df=35\n",
      "parameter F test:         F=0.9846  , p=0.5038  , df_denom=106, df_num=35\n",
      "\n",
      "Granger Causality\n",
      "number of lags (no zero) 36\n",
      "ssr based F test:         F=0.9089  , p=0.6182  , df_denom=103, df_num=36\n",
      "ssr based chi2 test:   chi2=55.9096 , p=0.0183  , df=36\n",
      "likelihood ratio test: chi2=48.5520 , p=0.0789  , df=36\n",
      "parameter F test:         F=0.9089  , p=0.6182  , df_denom=103, df_num=36\n",
      "\n",
      "Granger Causality\n",
      "number of lags (no zero) 37\n",
      "ssr based F test:         F=0.9010  , p=0.6318  , df_denom=100, df_num=37\n",
      "ssr based chi2 test:   chi2=58.3377 , p=0.0141  , df=37\n",
      "likelihood ratio test: chi2=50.3476 , p=0.0705  , df=37\n",
      "parameter F test:         F=0.9010  , p=0.6318  , df_denom=100, df_num=37\n",
      "\n",
      "Granger Causality\n",
      "number of lags (no zero) 38\n",
      "ssr based F test:         F=0.8434  , p=0.7188  , df_denom=97, df_num=38\n",
      "ssr based chi2 test:   chi2=57.4887 , p=0.0221  , df=38\n",
      "likelihood ratio test: chi2=49.6728 , p=0.0973  , df=38\n",
      "parameter F test:         F=0.8434  , p=0.7188  , df_denom=97, df_num=38\n",
      "\n",
      "Granger Causality\n",
      "number of lags (no zero) 39\n",
      "ssr based F test:         F=0.8616  , p=0.6942  , df_denom=94, df_num=39\n",
      "ssr based chi2 test:   chi2=61.8408 , p=0.0114  , df=39\n",
      "likelihood ratio test: chi2=52.8716 , p=0.0682  , df=39\n",
      "parameter F test:         F=0.8616  , p=0.6942  , df_denom=94, df_num=39\n",
      "\n",
      "Granger Causality\n",
      "number of lags (no zero) 40\n",
      "ssr based F test:         F=0.8416  , p=0.7251  , df_denom=91, df_num=40\n",
      "ssr based chi2 test:   chi2=63.6275 , p=0.0101  , df=40\n",
      "likelihood ratio test: chi2=54.1383 , p=0.0671  , df=40\n",
      "parameter F test:         F=0.8416  , p=0.7251  , df_denom=91, df_num=40\n",
      "\n",
      "Granger Causality\n",
      "number of lags (no zero) 41\n",
      "ssr based F test:         F=0.8305  , p=0.7425  , df_denom=88, df_num=41\n",
      "ssr based chi2 test:   chi2=66.1672 , p=0.0077  , df=41\n",
      "likelihood ratio test: chi2=55.9344 , p=0.0600  , df=41\n",
      "parameter F test:         F=0.8305  , p=0.7425  , df_denom=88, df_num=41\n",
      "\n",
      "Granger Causality\n",
      "number of lags (no zero) 42\n",
      "ssr based F test:         F=0.8211  , p=0.7571  , df_denom=85, df_num=42\n",
      "ssr based chi2 test:   chi2=68.9762 , p=0.0054  , df=42\n",
      "likelihood ratio test: chi2=57.8962 , p=0.0521  , df=42\n",
      "parameter F test:         F=0.8211  , p=0.7571  , df_denom=85, df_num=42\n",
      "\n",
      "Granger Causality\n",
      "number of lags (no zero) 43\n",
      "ssr based F test:         F=0.9422  , p=0.5770  , df_denom=82, df_num=43\n",
      "ssr based chi2 test:   chi2=83.4997 , p=0.0002  , df=43\n",
      "likelihood ratio test: chi2=67.8554 , p=0.0092  , df=43\n",
      "parameter F test:         F=0.9422  , p=0.5770  , df_denom=82, df_num=43\n",
      "\n",
      "Granger Causality\n",
      "number of lags (no zero) 44\n",
      "ssr based F test:         F=1.0135  , p=0.4698  , df_denom=79, df_num=44\n",
      "ssr based chi2 test:   chi2=94.8361 , p=0.0000  , df=44\n",
      "likelihood ratio test: chi2=75.1912 , p=0.0023  , df=44\n",
      "parameter F test:         F=1.0135  , p=0.4698  , df_denom=79, df_num=44\n",
      "\n",
      "Granger Causality\n",
      "number of lags (no zero) 45\n",
      "ssr based F test:         F=1.0682  , p=0.3935  , df_denom=76, df_num=45\n",
      "ssr based chi2 test:   chi2=105.6230, p=0.0000  , df=45\n",
      "likelihood ratio test: chi2=81.8460 , p=0.0006  , df=45\n",
      "parameter F test:         F=1.0682  , p=0.3935  , df_denom=76, df_num=45\n",
      "\n",
      "Granger Causality\n",
      "number of lags (no zero) 46\n",
      "ssr based F test:         F=1.1447  , p=0.2987  , df_denom=73, df_num=46\n",
      "ssr based chi2 test:   chi2=119.7388, p=0.0000  , df=46\n",
      "likelihood ratio test: chi2=90.1530 , p=0.0001  , df=46\n",
      "parameter F test:         F=1.1447  , p=0.2987  , df_denom=73, df_num=46\n",
      "\n",
      "Granger Causality\n",
      "number of lags (no zero) 47\n",
      "ssr based F test:         F=1.4858  , p=0.0655  , df_denom=70, df_num=47\n",
      "ssr based chi2 test:   chi2=164.6008, p=0.0000  , df=47\n",
      "likelihood ratio test: chi2=114.1696, p=0.0000  , df=47\n",
      "parameter F test:         F=1.4858  , p=0.0655  , df_denom=70, df_num=47\n",
      "\n",
      "Granger Causality\n",
      "number of lags (no zero) 48\n",
      "ssr based F test:         F=1.3478  , p=0.1286  , df_denom=67, df_num=48\n",
      "ssr based chi2 test:   chi2=158.3565, p=0.0000  , df=48\n",
      "likelihood ratio test: chi2=110.8298, p=0.0000  , df=48\n",
      "parameter F test:         F=1.3478  , p=0.1286  , df_denom=67, df_num=48\n",
      "\n",
      "Granger Causality\n",
      "number of lags (no zero) 49\n",
      "ssr based F test:         F=1.3982  , p=0.1036  , df_denom=64, df_num=49\n",
      "ssr based chi2 test:   chi2=174.4933, p=0.0000  , df=49\n",
      "likelihood ratio test: chi2=118.6306, p=0.0000  , df=49\n",
      "parameter F test:         F=1.3982  , p=0.1036  , df_denom=64, df_num=49\n",
      "\n",
      "Granger Causality\n",
      "number of lags (no zero) 50\n",
      "ssr based F test:         F=1.4995  , p=0.0657  , df_denom=61, df_num=50\n",
      "ssr based chi2 test:   chi2=199.1082, p=0.0000  , df=50\n",
      "likelihood ratio test: chi2=129.8562, p=0.0000  , df=50\n",
      "parameter F test:         F=1.4995  , p=0.0657  , df_denom=61, df_num=50\n",
      "{'chat': {'lag 10': 0.41337141862106197, 'lag 20': 0.736679240979448, 'lag 30': 0.5294251814859479, 'lag 40': 0.7250509439914312, 'lag 50': 0.06569606512445687}}\n",
      "chat :\n",
      "   p value :\n",
      "         lag 10 :  0.41337141862106197\n",
      "         lag 20 :  0.736679240979448\n",
      "         lag 30 :  0.5294251814859479\n",
      "         lag 40 :  0.7250509439914312\n",
      "         lag 50 :  0.06569606512445687\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# GUI for stock details\n",
    "comp = \"xxx\"\n",
    "keys =\"xxx\"\n",
    "class App(tk.Tk):\n",
    "\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        tk.Tk.__init__(self, *args, **kwargs)\n",
    "        self.title_font = tkfont.Font(family='Helvetica', size=18, weight=\"bold\", slant=\"italic\")\n",
    "\n",
    "        container = tk.Frame(self)\n",
    "        container.pack(side=\"top\", fill=\"both\", expand=True)\n",
    "        container.grid_rowconfigure(0, weight=1)\n",
    "        container.grid_columnconfigure(0, weight=1)\n",
    "\n",
    "        self.frames = {}\n",
    "        self.pages = (StartPage, PageOne, PageTwo)\n",
    "        for F in self.pages:\n",
    "            page_name = F.__name__\n",
    "            frame = F(parent=container, controller=self)\n",
    "            self.frames[page_name] = frame\n",
    "            frame.grid(row=0, column=0, sticky=\"nsew\")\n",
    "            frame.configure(background='#ffffff')\n",
    "\n",
    "        self.show_frame(\"StartPage\")\n",
    "\n",
    "    def show_frame(self, page_name):\n",
    "        '''Show a frame for the given page name'''\n",
    "        frame = self.frames[page_name]\n",
    "        frame.tkraise()\n",
    "        \n",
    "    def get_page(self, page_name):\n",
    "        for page in self.frames.values():\n",
    "            if str(page.__class__.__name__) == page_name:\n",
    "                return page\n",
    "        return None \n",
    "        \n",
    "\n",
    "# screen 1\n",
    "class StartPage(tk.Frame):\n",
    "\n",
    "    def __init__(self, parent, controller):\n",
    "        tk.Frame.__init__(self, parent)\n",
    "        self.controller = controller\n",
    "        l = tk.Label(self, text=\"Stock Details of a Company\",font='Sans-serif 30 bold',justify=tk.CENTER)\n",
    "        l.configure(background='#ffffff')\n",
    "        l.grid(row=0, column=0,columnspan=30,pady=(5,5), padx=(10,10))\n",
    "        \n",
    "        self.canvas = tk.Canvas(self, bg=\"white\", height=500, width=900)\n",
    "        self.canvas.grid(row=1,rowspan=10,padx=40)\n",
    "        \n",
    "        self.c1= tk.Canvas(self, bg=\"white\", height=170, width=320)\n",
    "        self.c1.grid(row=6,column=1,columnspan=5,padx=0, pady=4)\n",
    "        \n",
    "        self.c = tk.Canvas(self, bg=\"white\", height=170, width=320)\n",
    "        self.c.grid(row=8,column=1,columnspan=5,padx=0, pady=5)\n",
    "           \n",
    "        self.var = tk.StringVar(self)\n",
    "        self.var.set(random.choice(comp_list))\n",
    "        \n",
    "        lbl1 = tk.Label(self, text=\"Company\",font='Helvetica 12 bold')\n",
    "        lbl1.configure(background='#ffffff')\n",
    "        lbl1.grid(row=2,column=1,padx=0)\n",
    "\n",
    "        combo = Combobox(self,textvariable=self.var)\n",
    "        combo['values']= comp_list\n",
    "        combo.grid(row=2,column=2,padx=20)\n",
    "\n",
    "        lbl2 = tk.Label(self, text=\"Keywords\",font='Helvetica 12 bold')\n",
    "        lbl2.configure(background='#ffffff')\n",
    "        lbl2.grid(row=3,column=1,padx=20)\n",
    "\n",
    "        sb = tk.Scrollbar(self, orient=tk.VERTICAL)\n",
    "        global lb\n",
    "        lb = tk.Listbox(self,selectmode=tk.MULTIPLE, yscrollcommand=sb.set)\n",
    "        lb.insert(tk.END,*word_list)\n",
    "        sb.grid(row=4,column=2,sticky=(tk.NS,tk.W))\n",
    "        sb.config(command=lb.yview)\n",
    "        lb.grid(row=4,column=1,sticky=tk.E)\n",
    "\n",
    "        self.subbtn = tk.Button(self, text=\"submit\",fg='white', bg='#13ade0',relief=tk.GROOVE,activebackground='#1da1f2',activeforeground='white',command=self.select, width = 8)\n",
    "        self.subbtn.grid(row=4,column=2)\n",
    "\n",
    "        w = tk.Label(self, text=\"Test for Stationarity\", font=(\"Helvetica\", 16),justify=tk.LEFT)\n",
    "        w.configure(background='#ffffff')\n",
    "        w.grid(row=5,column=2)\n",
    "        \n",
    "        self.gotobtnright = tk.Button(self, state = tk.DISABLED, text=\"Go to Dashboard\",font='Helvetica 9 bold',fg='white', bg='#13ade0',relief=tk.GROOVE,activebackground='#1da1f2',activeforeground='white',command=lambda: controller.show_frame(\"PageOne\"),width=20)\n",
    "        self.gotobtnright.grid(row=11,column=0)\n",
    "    \n",
    "      \n",
    "        \n",
    "    def smooth_keys(self,keys):\n",
    "        temp = []\n",
    "        for word in keys:\n",
    "            word_count_data= data_words[data_words['keyword']==word]\n",
    "            word_count_data['Date'] = word_count_data['Date'].astype('datetime64[ns]')\n",
    "            word_count_data= word_count_data.set_index('Date')\n",
    "            word_count = pd.DataFrame(word_count_data.freq,columns={'freq'})\n",
    "            # Smoothing and signal extension\n",
    "            result = moving_average(word_count,3)\n",
    "            ext_result = extend_signal(result.freq)\n",
    "            ext_result=pd.DataFrame(ext_result,columns={'freq'})\n",
    "            # Scaling\n",
    "            scaled_word_count = pd.DataFrame(scaler.fit_transform(ext_result),columns={word})\n",
    "            scaled_word_count.set_index(word_count.index,inplace=True)\n",
    "            # Smoothing\n",
    "            result = moving_average(scaled_word_count,3)\n",
    "            ext_result = extend_signal(result[word])\n",
    "            ext_result=pd.DataFrame(ext_result,columns={word})\n",
    "            temp.append(ext_result)\n",
    "        return temp\n",
    "        \n",
    "    # graph plot\n",
    "    def plot(self,company,keys):\n",
    "        cp = pd.DataFrame(stocks_data_2016[stocks_data_2016.Company==company].Close,columns={'Close'})\n",
    "        scaled_cp = pd.DataFrame(scaler.fit_transform(cp),columns={'Close'})\n",
    "        scaled_cp.set_index(cp.index,inplace=True)\n",
    "        \n",
    "\n",
    "        temp = self.smooth_keys(keys)\n",
    "\n",
    "        plt.figure(figsize = (10,5))\n",
    "        plt.plot(scaled_cp,label=company)\n",
    "        for i,word in zip(temp,keys):\n",
    "            plt.plot(i,label=word)\n",
    "        plt.title(\"company : \"+ company,fontsize=20)\n",
    "        plt.xlabel(\"Date\")\n",
    "        plt.legend(loc = 4)\n",
    "        if not os.path.exists(\"graph_images\"):\n",
    "            os.mkdir(\"graph_images/\")\n",
    "        PATH = \"graph_images/\"+company+\".jpeg\"\n",
    "        plt.savefig(PATH)\n",
    "        plt.cla()\n",
    "        plt.close('all')\n",
    "        return PATH,cp\n",
    "    \n",
    "    \n",
    "    def select(self):\n",
    "        self.gotobtnright.config(state = \"normal\")\n",
    "        def insert_into_entry():\n",
    "            items = list(map(int, lb.curselection()))\n",
    "            if items != ():\n",
    "                global selected_item\n",
    "                selected_item=[lb.get(items[i]) for i in range(len(items))]\n",
    "                return (selected_item)\n",
    "        global comp\n",
    "        comp = self.var.get()\n",
    "        global keys\n",
    "        keys = insert_into_entry()\n",
    "    \n",
    "        path,cp = self.plot(comp,keys)\n",
    "        self.canvas.delete(\"all\")\n",
    "        self.c.delete('all')\n",
    "        self.c1.delete('all')\n",
    "        # graph\n",
    "        image = Image.open(path)\n",
    "        img = ImageTk.PhotoImage(image)\n",
    "        self.canvas.create_image(450,250, image=img)\n",
    "        label = tk.Label(image=img)\n",
    "        label.image = img \n",
    "        image.close()\n",
    "\n",
    "        # Dickey Fuller test and KPSS test\n",
    "        df_res = aDickeyFuller(cp.Close)\n",
    "        kpss_res = kpss_test(cp.Close)\n",
    "\n",
    "        # dfuller\n",
    "        self.c1.create_text(58,15,fill=\"Black\",font=\"Times 10 bold\",\n",
    "                                text=\"Dickey Fuller Test\", justify = \"center\")\n",
    "        Text = \"\\n\".join([\"{0} :  {1}\".format(key, df_res[key]) for key in df_res if key != \"Critical values\"])\n",
    "        for key in df_res[\"Critical values\"]:\n",
    "            Text += (\"\\n \\t {0} :  {1}\".format(key, df_res[\"Critical values\"][key]))\n",
    "        self.c1.create_text(135, 80, fill = \"Black\", font = \"Times 10\", text = Text, justify='left')\n",
    "        \n",
    "        #kpss\n",
    "        self.c.create_text(38,15,fill=\"Black\",font=\"Times 10 bold\",\n",
    "                                text=\"KPSS Test\",justify = \"center\")\n",
    "        Text_kpss = \"\\n\".join([\"{0} :  {1}\".format(key, kpss_res[key]) for key in kpss_res if key != \"Critical values\"])\n",
    "        for key in kpss_res[\"Critical values\"]:\n",
    "            Text_kpss += (\"\\n \\t {0} :  {1}\".format(key, kpss_res[\"Critical values\"][key]))\n",
    "        self.c.create_text(135, 80, fill = \"Black\", font = \"Times 10\", text = Text_kpss, justify='left')\n",
    "        \n",
    "        \n",
    "        \n",
    "\n",
    "\n",
    "# screen 2        \n",
    "class PageOne(tk.Frame):\n",
    "\n",
    "    def __init__(self, parent, controller):\n",
    "        tk.Frame.__init__(self, parent)\n",
    "        self.controller = controller\n",
    "       \n",
    "        self.canvas = tk.Canvas(self, bg=\"white\", height=480, width=900)\n",
    "        self.canvas.grid(row=0,rowspan=15,columnspan=10,padx=40,pady=5)\n",
    "        \n",
    "        self.c= tk.Canvas(self, bg=\"white\", height=170, width=320)\n",
    "        self.c.grid(row=0,rowspan=5,column=11,padx=10, pady=15)\n",
    "        \n",
    "        self.c1 = tk.Canvas(self, bg=\"white\", height=170, width=320)\n",
    "        self.c1.grid(row=6,rowspan=5,column=11,padx=10, pady=15)\n",
    "        \n",
    "        l1 = tk.Label(self, text=\"Granger Test\",font='Helvetica 11 bold',justify=tk.CENTER)\n",
    "        l1.configure(background='#ffffff')\n",
    "        l1.grid(row=11,rowspan=2,column=11,pady=0)\n",
    "\n",
    "        vbar=tk.Scrollbar(self,orient=tk.VERTICAL)\n",
    "        self.c2 = tk.Canvas(self, bg=\"white\", height=220, width=320,yscrollcommand=vbar.set)\n",
    "        self.c2.grid(row=13,rowspan=5,column=11,padx=0, pady=5,sticky=tk.E)\n",
    "        win = tk.Frame(self.c2)\n",
    "        self.c2.create_window(0,0, window = win, anchor = \"nw\")\n",
    "        win.update_idletasks()\n",
    "        self.c2.configure(scrollregion = (1,1,win.winfo_width(),win.winfo_height()))\n",
    "\n",
    "        vbar.grid(row=13,rowspan=5,column=12,sticky=(tk.NS,tk.W))\n",
    "        vbar.config(command=self.c2.yview)\n",
    "        \n",
    "        \n",
    "        self.c3 = tk.Canvas(self, bg=\"white\", height=130, width=320)\n",
    "        self.c3.grid(row=16,rowspan=3,column=8,padx=0, pady=5)\n",
    "        \n",
    "        l2 = tk.Label(self, text=\"Data Transformation\",font='Helvetica 11 bold',justify=tk.CENTER)\n",
    "        l2.configure(background='#ffffff')\n",
    "        l2.grid(row=16,column=1)\n",
    "        \n",
    "        tf_methods = ['Raw Data','Log Transformation','1st order Differencing','2nd order Differencing']\n",
    "        self.var = tk.StringVar(self)\n",
    "        self.var.set(tf_methods[0])\n",
    "        \n",
    "        combo = Combobox(self,textvariable=self.var,width=25)\n",
    "        combo['values']= tf_methods\n",
    "        combo.grid(row=16,column=2,padx=20)\n",
    "        \n",
    "        l3 = tk.Label(self, text=\"Smoothing techniques\",font='Helvetica 11 bold',justify=tk.CENTER)\n",
    "        l3.configure(background='#ffffff')\n",
    "        l3.grid(row=17,column=1,pady=30)\n",
    "        \n",
    "        smooth_methods = ['Exponential Smoothing','Simple Exponential Smoothing','Moving Average']\n",
    "        sm_fun = [exp_smoothing, simple_exp_smoothing, moving_average]\n",
    "        self.sm_dict ={}\n",
    "        for m,fun in zip(smooth_methods,sm_fun):\n",
    "            self.sm_dict[m] = fun     \n",
    "        self.var1 = tk.StringVar(self)\n",
    "        self.var1.set(smooth_methods[0])\n",
    "        \n",
    "        combo1 = Combobox(self,textvariable=self.var1,width=25)\n",
    "        combo1['values']= smooth_methods\n",
    "        combo1.grid(row=17,column=2,padx=20,pady=30)\n",
    "        \n",
    "        l4 = tk.Label(self, text=\"Filter techniques\",font='Helvetica 11 bold',justify=tk.CENTER)\n",
    "        l4.configure(background='#ffffff')\n",
    "        l4.grid(row=18,column=1)\n",
    "        \n",
    "        filter_methods = ['Baxter King','Hodrick Prescott','Random Walk']\n",
    "        fm_fun = [baxter_king, hodrick_prescott, random_walk_filter]\n",
    "        self.fm_dict ={}\n",
    "        for m,fun in zip(filter_methods,fm_fun):\n",
    "            self.fm_dict[m] = fun\n",
    "        self.var2 = tk.StringVar(self)\n",
    "        self.var2.set(filter_methods[0])\n",
    "        \n",
    "        combo2 = Combobox(self,textvariable=self.var2,width=25)\n",
    "        combo2['values']= filter_methods\n",
    "        combo2.grid(row=18,column=2,padx=20)\n",
    "        \n",
    "        subbtn = tk.Button(self, text=\"submit\",font='Helvetica 9 bold',fg='white', bg='#13ade0',relief=tk.GROOVE,activebackground='#1da1f2',activeforeground='white',command=self.selectvalue,width=8)\n",
    "        subbtn.grid(row=17,column=5)\n",
    "        \n",
    "        gotobtnleft = tk.Button(self, text=\"<\",font='Helvetica 9 bold',fg='white', bg='#13ade0',relief=tk.GROOVE,activebackground='#1da1f2',activeforeground='white',command=lambda: controller.show_frame(\"StartPage\"),width=5)\n",
    "        gotobtnleft.grid(row=19,column=5)\n",
    "        \n",
    "        self.gotobtnright = tk.Button(self, text=\">\",font='Helvetica 9 bold',fg='white', bg='#13ade0',relief=tk.GROOVE,activebackground='#1da1f2',activeforeground='white',command=lambda: controller.show_frame(\"PageTwo\"),width=5)\n",
    "        self.gotobtnright.grid(row=19,column=6)\n",
    "     \n",
    "    def impute_keycount(self,keys,cp):\n",
    "        \n",
    "        temp = []\n",
    "        for word in keys:\n",
    "            word_count_data= data_words[data_words['keyword']==word]\n",
    "            word_count_data['Date'] = word_count_data['Date'].astype('datetime64[ns]')\n",
    "            word_count_data= word_count_data.set_index('Date')\n",
    "            word_count = pd.DataFrame(word_count_data.freq,columns={'freq'})\n",
    "            date=[i.split()[0] for i in list(map(str,cp.index))]\n",
    "            word_count = impute_points(word_count,date)\n",
    "\n",
    "            # Smoothing and signal extension\n",
    "            result = moving_average(word_count,3)\n",
    "            ext_result = extend_signal(result.freq)\n",
    "            ext_result=pd.DataFrame(ext_result,columns={'freq'})\n",
    "            # Scaling\n",
    "            scaled_word_count = pd.DataFrame(scaler.fit_transform(ext_result),columns={word})\n",
    "            scaled_word_count.set_index(word_count.index,inplace=True)\n",
    "            # Smoothing\n",
    "            result = moving_average(scaled_word_count,3)\n",
    "            ext_result = extend_signal(result[word])\n",
    "            ext_result=pd.DataFrame(ext_result,columns={word})\n",
    "            \n",
    "            temp.append(ext_result)\n",
    "            \n",
    "        return temp\n",
    "        \n",
    "    \n",
    "    def plotvalues(self,company,cp,keys):\n",
    "                \n",
    "        startpage = self.controller.get_page('StartPage')\n",
    "        temp = self.impute_keycount(keys,cp)\n",
    "\n",
    "        plt.figure(figsize = (10,5))\n",
    "        plt.plot(cp,label=company)\n",
    "        for i,word in zip(temp,keys):\n",
    "            plt.plot(i,label=word)\n",
    "        plt.title(\"company : \"+ company,fontsize=20)\n",
    "        plt.xlabel(\"Date\")\n",
    "        plt.legend(loc = 4)\n",
    "        if not os.path.exists(\"graph_images\"):\n",
    "            os.mkdir(\"graph_images/\")\n",
    "        PATH = \"graph_images/\"+company+\".jpeg\"\n",
    "        plt.savefig(PATH)\n",
    "        plt.cla()\n",
    "        plt.close('all')\n",
    "        return PATH,temp\n",
    "        \n",
    "        \n",
    "        \n",
    "    def selectvalue(self):\n",
    "        \n",
    "        cp = pd.DataFrame(stocks_data_2016[stocks_data_2016.Company==comp].Close,columns={'Close'})\n",
    "        tfm = self.var.get()\n",
    "        sm = self.var1.get()\n",
    "        fm = self.var2.get()\n",
    "        \n",
    "        temp_cp = deepcopy(cp)\n",
    "        if tfm == 'Log Transformation':\n",
    "            temp_cp = np.log(cp.Close)\n",
    "            \n",
    "        elif tfm == '1st order Differencing':\n",
    "            temp_cp = cp.Close - cp.Close.shift()\n",
    "        elif tfm == '2nd order Differencing':\n",
    "            temp_cp = cp.Close - 2*cp.Close.shift()+ cp.Close.shift(periods=2)\n",
    "            temp_cp = extend_signal(temp_cp)\n",
    "        \n",
    "        temp_cp = self.sm_dict[sm](temp_cp)\n",
    "        temp_cp.rename(columns={0 : 'Close'},inplace=True)\n",
    "        temp_cp = extend_signal(temp_cp.Close)\n",
    "        temp_cp = self.fm_dict[fm](temp_cp)\n",
    "        temp_cp = pd.DataFrame(temp_cp,columns=['Close'])\n",
    "        \n",
    "        # Scaling\n",
    "        scaled_cp = pd.DataFrame(scaler.fit_transform(temp_cp),columns={'Close'})\n",
    "        scaled_cp.set_index(temp_cp.index,inplace=True)\n",
    "        \n",
    "        path,imputed_key = self.plotvalues(comp,scaled_cp,keys)\n",
    "        self.canvas.delete(\"all\")\n",
    "        self.c.delete('all')\n",
    "        self.c1.delete('all')\n",
    "        self.c2.delete('all')\n",
    "        self.c3.delete('all')\n",
    "        \n",
    "        # graph\n",
    "        image = Image.open(path)\n",
    "        img = ImageTk.PhotoImage(image)\n",
    "        self.canvas.create_image(450,230, image=img)\n",
    "        label = tk.Label(image=img)\n",
    "        label.image = img \n",
    "        image.close()\n",
    "        \n",
    "        \n",
    "        # Dickey Fuller test and KPSS test\n",
    "        df_res = aDickeyFuller(scaled_cp.Close)\n",
    "        kpss_res = kpss_test(scaled_cp.Close)\n",
    "        \n",
    "        # dfuller\n",
    "        self.c.create_text(58,15,fill=\"Black\",font=\"Times 10 bold\",\n",
    "                                text=\"Dickey Fuller Test\", justify = \"center\")\n",
    "        Text = \"\\n\".join([\"{0} :  {1}\".format(key, df_res[key]) for key in df_res if key != \"Critical values\"])\n",
    "        for key in df_res[\"Critical values\"]:\n",
    "            Text += (\"\\n \\t {0} :  {1}\".format(key, df_res[\"Critical values\"][key]))\n",
    "        self.c.create_text(135, 80, fill = \"Black\", font = \"Times 10\", text = Text, justify='left')\n",
    "        \n",
    "        #kpss\n",
    "        self.c1.create_text(38,15,fill=\"Black\",font=\"Times 10 bold\",\n",
    "                                text=\"KPSS Test\",justify = \"center\")\n",
    "        Text_kpss = \"\\n\".join([\"{0} :  {1}\".format(key, kpss_res[key]) for key in kpss_res if key != \"Critical values\"])\n",
    "        for key in kpss_res[\"Critical values\"]:\n",
    "            Text_kpss += (\"\\n \\t {0} :  {1}\".format(key, kpss_res[\"Critical values\"][key]))\n",
    "        self.c1.create_text(135, 80, fill = \"Black\", font = \"Times 10\", text = Text_kpss, justify='left')\n",
    "        \n",
    "        RMSE={}\n",
    "        res_granger = {}\n",
    "        for key_data,word in zip(imputed_key,keys):\n",
    "            #print(key_data.isna().sum())\n",
    "            x=pd.concat([scaled_cp,key_data],axis=1)\n",
    "            RMSE[word]=rmse(x.Close,x[word])\n",
    "            lags=50\n",
    "            try:                \n",
    "                res = granger_test(x)\n",
    "                pvalues = {}\n",
    "                for i in range(10,lags+1,10):\n",
    "                    pvalues[\"lag \"+str(i)]=res[i][0]['ssr_ftest'][1]\n",
    "                res_granger[word]=pvalues \n",
    "            except Exception as e:\n",
    "                lags = int(str(e).split()[-1])\n",
    "                print(str(e))\n",
    "                print(lags)\n",
    "                \n",
    "                res = granger_test(x,lags)\n",
    "                pvalues = {}\n",
    "                s = 1\n",
    "                e = lags+1\n",
    "                if lags < 10 :\n",
    "                    skip = 2\n",
    "                elif lags < 30:\n",
    "                    skip = 4\n",
    "                elif lags < 50:\n",
    "                    skip = 10          \n",
    "                for i in range(s,lags+1,skip):\n",
    "                    pvalues[\"lag \"+str(i)]=res[i][0]['ssr_ftest'][1]\n",
    "                res_granger[word]=pvalues\n",
    "                \n",
    "            \n",
    "        print(res_granger)\n",
    "\n",
    "        \n",
    "        #RMSE\n",
    "        self.c3.create_text(90,15,fill=\"Black\",font=\"Times 10 bold\",\n",
    "                                text=\"Root Mean Square Error\",justify = \"center\")\n",
    "        Text_rmse = \"\\n\".join([\"{0} :  {1}\".format(key, RMSE[key]) for key in RMSE])\n",
    "        self.c3.create_text(105, 70, fill = \"Black\", font = \"Times 10\", text =Text_rmse , justify='left')\n",
    "        \n",
    "        #Granger test\n",
    "        Text_rmse=\"\"\n",
    "        self.c2.create_text(150,20, fill = \"Black\", font = \"Times 11 bold\", text = \"*Lower is better*\", justify='left')\n",
    "        for word in keys:\n",
    "            Text_rmse += word+\" :\\n\"\n",
    "            Text_rmse+= \"   p value :\\n\"\n",
    "            Text_rmse += \"\\n\".join([\"         {0} :  {1}\".format(lag, res_granger[word][lag]) for lag in res_granger[word]])\n",
    "            Text_rmse+=\"\\n\"\n",
    "        print(Text_rmse)\n",
    "        self.c2.create_text(120, 120, fill = \"Black\", font = \"Times 10\", text =Text_rmse , justify='left')\n",
    "\n",
    "\n",
    "        \n",
    "        \n",
    "\n",
    "# screen 3\n",
    "class PageTwo(tk.Frame):\n",
    "\n",
    "    def __init__(self, parent, controller):\n",
    "        tk.Frame.__init__(self, parent)\n",
    "        self.controller = controller\n",
    "        \n",
    "        self.rmse = {}\n",
    "        l = tk.Label(self, text=\"Stock Prediction\",font='Sans-serif 30 bold',justify=tk.CENTER)\n",
    "        l.configure(background='#ffffff')\n",
    "        l.grid(row=0, column=0,columnspan=30,pady=(5,5), padx=(10,10))\n",
    "        \n",
    "        self.canvas = tk.Canvas(self, bg=\"white\", height=550, width=1000)\n",
    "        self.canvas.grid(row=1,rowspan=15,columnspan=10,padx=35,pady=20)\n",
    "        \n",
    "        self.c = tk.Canvas(self, bg=\"white\", height=120, width=260)\n",
    "        self.c.grid(row=10,rowspan=3,column=12,padx=0, pady=5)\n",
    "        \n",
    "        lstmbtn = tk.Button(self, text=\"LSTM\",font='Helvetica 9 bold',fg='white', bg='#13ade0',relief=tk.GROOVE,activebackground='#1da1f2',activeforeground='white',command=self.lstm,width=20)\n",
    "        lstmbtn.grid(row=5,column=12)\n",
    "        \n",
    "        prophetbtn = tk.Button(self, text=\"Prophet\",font='Helvetica 9 bold',fg='white', bg='#13ade0',relief=tk.GROOVE,activebackground='#1da1f2',activeforeground='white',command=self.prophet,width=20)\n",
    "        prophetbtn.grid(row=6,column=12)\n",
    "        \n",
    "        arimabtn = tk.Button(self, text=\"AUTO_ARIMAX\",font='Helvetica 9 bold',fg='white', bg='#13ade0',relief=tk.GROOVE,activebackground='#1da1f2',activeforeground='white',command=self.auto_arimax,width=20)\n",
    "        arimabtn.grid(row=7,column=12)\n",
    "        \n",
    "        lrbtn = tk.Button(self, text=\"Linear Regression\",font='Helvetica 9 bold',fg='white', bg='#13ade0',relief=tk.GROOVE,activebackground='#1da1f2',activeforeground='white',command=self.linear_regression,width=20)\n",
    "        lrbtn.grid(row=8,column=12)\n",
    "\n",
    "        \n",
    "        \n",
    "        gotobtnleft = tk.Button(self, text=\"<\",font='Helvetica 9 bold',fg='white', bg='#13ade0',relief=tk.GROOVE,activebackground='#1da1f2',activeforeground='white',command=lambda: controller.show_frame(\"PageOne\"),width=5)\n",
    "        gotobtnleft.grid(row=17,column=5)\n",
    "        \n",
    "    def load_image(self,path):\n",
    "        # graph\n",
    "        \n",
    "        self.canvas.delete('all')\n",
    "        image = Image.open(path)\n",
    "        #image.show()\n",
    "        self.image = ImageTk.PhotoImage(image)\n",
    "        self.img_id = self.canvas.create_image(480,300, image=self.image)\n",
    "        self.canvas.update_idletasks()\n",
    "        label = tk.Label(image=self.image)\n",
    "        label.image = self.image \n",
    "        image.close()\n",
    "        \n",
    "    def create_canvas_img(self,rmse,company,method=\"\"):\n",
    "        PATH = \"graph_images/\"+company+\"_\"+method+\".jpeg\"\n",
    "            \n",
    "        plt.title(\"company : \"+ company,fontsize=10)\n",
    "        plt.xlabel(\"Date\")\n",
    "        plt.legend()\n",
    "        fig = plt.gcf()\n",
    "        fig.set_size_inches(10,5)\n",
    "        plt.savefig(PATH)\n",
    "        plt.cla()\n",
    "        plt.close('all') \n",
    "        \n",
    "        self.canvas.delete('all')\n",
    "        self.c.delete(\"all\")\n",
    "        # graph\n",
    "        image = Image.open(PATH)\n",
    "        self.img = ImageTk.PhotoImage(image)\n",
    "        self.canvas.create_image(480,300, image=self.img)\n",
    "        label = tk.Label(image=self.img)\n",
    "        label.image = self.img \n",
    "        image.close()\n",
    "        \n",
    "        #RMSE\n",
    "        self.c.create_text(90,15,fill=\"Black\",font=\"Times 10 bold\",\n",
    "                                text=\"Root Mean Square Error\",justify = \"center\")\n",
    "       \n",
    "        self.c.create_text(105, 50, fill = \"Black\", font = \"Times 10\", text =rmse , justify='left')\n",
    "\n",
    "    \n",
    "    def scale_cp(self,cp):\n",
    "        cp =np.log(cp)\n",
    "        scaled_cp = pd.DataFrame(scaler.fit_transform(cp),columns={'Close'})\n",
    "        scaled_cp.set_index(cp.index,inplace=True)\n",
    "        return scaled_cp\n",
    "    \n",
    "    def is_img_exists(self,PATH,rmse):\n",
    "        self.canvas.delete('all')\n",
    "        self.c.delete(\"all\")\n",
    "        # graph\n",
    "        image = Image.open(PATH)\n",
    "        self.img = ImageTk.PhotoImage(image)\n",
    "        self.canvas.create_image(480,300, image=self.img)\n",
    "        label = tk.Label(image=self.img)\n",
    "        label.image = self.img \n",
    "        image.close()\n",
    "        \n",
    "        #RMSE\n",
    "        self.c.create_text(90,15,fill=\"Black\",font=\"Times 10 bold\",\n",
    "                                text=\"Root Mean Square Error\",justify = \"center\")\n",
    "       \n",
    "        self.c.create_text(105, 50, fill = \"Black\", font = \"Times 10\", text =rmse , justify='left')\n",
    "\n",
    "        \n",
    "    def linear_regression(self):\n",
    "        \n",
    "        stocks_data_2016_end = stocks_data['2016':]\n",
    "        cp = pd.DataFrame(stocks_data_2016_end[stocks_data_2016_end.Company==comp].Close,columns={'Close'})\n",
    "        scaled_cp =self.scale_cp(cp)\n",
    "        PATH = \"loading.jpg\"\n",
    "        grp_img_path = \"graph_images/\"+comp+\"_lr\"+\".jpeg\"\n",
    "        if not os.path.exists(grp_img_path):\n",
    "            self.load_image(PATH)\n",
    "            self.rmse['lr'] = regression_model(scaled_cp)\n",
    "            self.create_canvas_img(self.rmse['lr'],comp,\"lr\")\n",
    "        else:\n",
    "            self.is_img_exists(grp_img_path,self.rmse['lr'])\n",
    "            \n",
    "        \n",
    "        \n",
    "    def auto_arimax(self):\n",
    "        stocks_data_2016_end = stocks_data['2016':]\n",
    "        cp = pd.DataFrame(stocks_data_2016_end[stocks_data_2016_end.Company==comp].Close,columns={'Close'})\n",
    "        scaled_cp =self.scale_cp(cp)\n",
    "        PATH = \"loading.jpg\"\n",
    "        grp_img_path = \"graph_images/\"+comp+\"_arimax\"+\".jpeg\"\n",
    "        if not os.path.exists(grp_img_path):\n",
    "            self.load_image(PATH)\n",
    "            self.rmse['arimax'] = auto_arimax(scaled_cp)\n",
    "            self.create_canvas_img(self.rmse['arimax'],comp,\"arimax\")\n",
    "        else:\n",
    "            self.is_img_exists(grp_img_path,self.rmse['arimax'])\n",
    "    \n",
    "    def lstm(self):\n",
    "        stocks_data_2016_end = stocks_data['2016':]\n",
    "        cp = pd.DataFrame(stocks_data_2016_end[stocks_data_2016_end.Company==comp].Close,columns={'Close'})\n",
    "        scaled_cp =self.scale_cp(cp)\n",
    "        PATH = \"loading.jpg\"\n",
    "        grp_img_path = \"graph_images/\"+comp+\"_lstm\"+\".jpeg\"\n",
    "        if not os.path.exists(grp_img_path):\n",
    "            self.load_image(PATH)\n",
    "            self.rmse['lstm'] = shallow_lstm(scaled_cp)\n",
    "            self.create_canvas_img(self.rmse['lstm'],comp,\"lstm\")\n",
    "        else:\n",
    "            self.is_img_exists(grp_img_path,self.rmse['lstm'])\n",
    "    \n",
    "        \n",
    "\n",
    "    def prophet(self):\n",
    "        stocks_data_2016_end = stocks_data['2016':]\n",
    "        cp = pd.DataFrame(stocks_data_2016_end[stocks_data_2016_end.Company==comp].Close,columns={'Close'})\n",
    "        scaled_cp =self.scale_cp(cp)\n",
    "        PATH = \"loading.jpg\"\n",
    "        grp_img_path = \"graph_images/\"+comp+\"_p\"+\".jpeg\"\n",
    "        if not os.path.exists(grp_img_path):\n",
    "            self.load_image(PATH)\n",
    "            self.rmse['p'] = prophet(scaled_cp)\n",
    "            self.create_canvas_img(self.rmse['p'],comp,\"p\")\n",
    "        else:\n",
    "            self.is_img_exists(grp_img_path,self.rmse['p'])\n",
    "        \n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    window = App()\n",
    "    window.configure(background='#ffffff')\n",
    "    w, h = window.winfo_screenwidth(), window.winfo_screenheight()\n",
    "    window.geometry(\"%dx%d+0+0\" % (w, h))\n",
    "    window.title(\"Stock Details\")\n",
    "    window.mainloop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
